---
title: "maxent model avec xvalidation (year-based test training)"
author: "Samantha Andrews"
output: html_notebook
---

# Overview
A note to anyone who might happen to stumble across this... I am a beginner in R and have had no exposure to similar languages. I don't know what I'm doing. The code herein is unlikely to be elegant and there area probably more efficient ways of running the code.

Built with 'r getRversion()'..

# Package dependencies
You can install and load them using the following code which uses a function called [ipak](https://gist.github.com/stevenworthington/3178163). Note this function checks to see if the packages are installed first before loading.


```{r pre-install & load packages, include=FALSE}
options(java.parameters = "-Xmx1g" )#needs to run before dismo loaded - boosts memory for java to 1g
packages <- c("dismo", "stringr", "rJava", "raster", "ecospat")
source("../src/ipak.R")
ipak(packages)
```


#load the presence-background dataset 
("../output/bio/presab_nafo_yr.csv")

```{r load pres-background dataset}
presback <- read.csv("../output/bio/presab_nafo_yr.csv", header = TRUE)

#remove unnecessary columns for maxent
presback <- subset(presback, select = -c(cell_id, id, decimalLatitude, decimalLongitude, datecollected, institutioncode, individualcount, depth, resname, originalscientificname, collectioncode, day, nafo_zone, gear, longitude_meters, latitude_meters, total_cell_obs_xy, total_cell_obs_xyt, bottom_depth, XXtotal_cell_obs_xyzt, temp_celsius_depth, temp_celsius_surface, longitude_meters.1, latitude_meters.1, bottom_depth_glorys, longitude_meters.2, latitude_meters.2, cell_id_3d, total_cell_obs_xyzt, cell_id_xyzt))
colnames(presback)
```

this dataset contains a lot of columns you don't need for the maxent models. Remove them here


#monthly model data selection and prep
For each monthly model, subset the data to the month you want, and the variables you want (Based on spearmans and VIF)

```{r}
janvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface")
marvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
aprvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
mayvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
junvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
julvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
augvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
sepvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
octvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
novvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")
decvar <- c("occurrence", "year", "temp_depth", "salinity_depth", "o2_depth", "chl_surface", "nao_sample", "nao_prev", "nao_winter", "amo_sample", "amo_winter")

mthnavar_lst <- list(jan = janvar, mar = marvar, apr = aprvar, may = mayvar, jun = junvar, jul = julvar, aug = augvar, sep = sepvar, oct = octvar, nov = novvar, dec = decvar)
```

select mont
```{r month of interest -change}
mthna <- "jan"
mthno <- "1"
```



```{r subsest data by month}
prback <- subset(presback, month == mthno)
prback <- subset(prback, select = mthnavar_lst[[mthna]])
```

now each month contains NAs - lets remove these (maxent principle doesnt allow for missing data - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.5607&rep=rep1&type=pdf)

```{r remove na vals}
prback <- na.omit(prback)
```


normal model..
mod12 <- maxent(x = prab, p = pa, file = .asc, args=c("maximumiterations=5000", "responsecurves=TRUE", "jackknife=TRUE", "threads=8", "addsamplestobackground=TRUE", "replicates=5", "replicatetype=crossvalidate")) 
mod12

```{r train and test inc AIC for reg val}
modelmonth <- cbind(paste0(mthna ,unique(prback$year)))
#modelmonth <- "jan2003"
aicetc <- read.csv("../data/env/AICcCalculations_p2.csv", header = TRUE) #pre-created csv file
betavallist <- read.csv("../data/env/AICcCalculations.csv", header = TRUE) #pre-created csv file
betavallist <- cbind(betavallist$regularization)
nfolds <- cbind(c(1:5))

for (year in 1:nrow(modelmonth)){
  
  timeslice <- modelmonth[year] #testing timeslice
  #timeslice <- modelmonth
  yr <- str_sub(timeslice, -4) #the year being used for testing
  train <- subset(prback, year == yr) #training data is all years but testing
  trainpa <- train$occurrence #creates a vector of presence/background ID (1/0)
  train  <- subset(train, select = -c(year, occurrence)) #don't need year and occurrence for modelling
  
  for (h in 1:nrow(betavallist)){
    betamval <-  betavallist[h]
    mod <- maxent(x = train, p = trainpa, args=c("maximumiterations=5000", "responsecurves=TRUE", "jackknife=TRUE", "threads=8", "addsamplestobackground=TRUE", paste0("replicates=", nrow(nfolds)), "replicatetype=crossvalidate", paste0("betamultiplier=", betamval)), path = paste0("C:/Users/bunny/Documents/GitHub/Chapter_1/output/maxent/p2/models/", timeslice)) #here the full path is needed
    
      
    #now based on glover-kapfer pdf with excel manual + https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-1171.1... + adaptation for        getting testAUC.. horrible code
   
    maxresults <- read.csv(paste0("../output/maxent/p2/models/", timeslice, "/maxentResults.csv"), header = TRUE)


    #count number of non-zero coefficients, excluding the final four rows
    lambdas_list <- list.files(paste0("../output/maxent/p2/models/", timeslice), pattern = "*lambdas", full.names = TRUE)
    samppred_list <- list.files(paste0("../output/maxent/p2/models/", timeslice), pattern = "*samplePredictions.csv", full.names = TRUE)
   
      for(f in 1:nrow(nfolds)){
      
      modn <- nfolds[f]-1
      lamf <- lambdas_list[f]
      sampf <- samppred_list[f]
      coefile <- read.csv(paste0(lamf), header = FALSE, stringsAsFactors=FALSE)
      coefile <- head(coefile, -4)
      coefile2 <- subset(coefile, V2 != 0)
      nocoeff <- nrow(coefile2)
      bick <- sum(as.numeric(coefile2[, 2])) #dodgy 
      bic <- (bick*log(sum(trainpa == 1)) - 2*11) #dodgy
      #natural log
      samplepred <- read.csv(paste0(sampf), header = TRUE)
      samplepred$nat_log <- log(samplepred$Raw.prediction)
      sumsamplepred <- sum(samplepred$nat_log)
   
      #no presences used in model (less those used for testing)

      notrainingpoints <- maxresults$X.Training.samples[f]
      testauc <- maxresults$Test.AUC[f]
      trainauc <- maxresults$Training.AUC[f]
      modno <- maxresults$Species[f]
    

      aicetc$train_AUC <- trainauc
      aicetc$test_AUC <- testauc
      aicetc$n <- notrainingpoints
      aicetc$coefficients <- nocoeff
      aicetc$sum.of.logs <- sumsamplepred
  
      aicetc$bic <- bic #not sure this is correct...
      aicetc$beta <- betamval
      aicetc$foldno <- modn
      
    aicetc$AIC <- ((2*aicetc$coefficients)-(2*aicetc$sum.of.logs))
    aicetc$AICc <- (-2*(aicetc$sum.of.logs)+((2*aicetc$coefficients)*(aicetc$n/(aicetc$n - aicetc$coefficients-1))))
    
    #calculate minimal predicted area using ecospat
    aicetc$mpa_0.9_cum <- ecospat.mpa(samplepred$Cumulative.prediction, perc = 0.9) #90%
    
    aicetc$AIC <- ((2*aicetc$coefficients)-(2*aicetc$sum.of.logs))
    aicetc$AICc <- (-2*(aicetc$sum.of.logs)+((2*aicetc$coefficients)*(aicetc$n/(aicetc$n - aicetc$coefficients-1))))
    write.csv(aicetc, paste0("../output/maxent/p2/models/", timeslice, "/", betamval, "_", modn, "_", "AICforMaxentReg.csv"), row.names = FALSE)
      }
    
  # aa <- paste0("fold", modn, "_", betamval, "_", "AICforMaxentReg.csv")
    aicperbetalst <- list.files(paste0("../output/maxent/p2/models/", timeslice, "/"), pattern = paste0("^", betamval, ".*AICforMaxentReg.csv"), full.names = TRUE)
    aicperbeta <- lapply(aicperbetalst, read.csv, header = TRUE)
    aicperbeta <- do.call(rbind, aicperbeta)
    aicbetamean <- colMeans(aicperbeta)
    aicperbeta <- rbind(aicperbeta, aicbetamean)
    aicperbeta$foldno[f+1] <- f+1
    write.csv(aicperbeta, paste0("../output/maxent/p2/models/", timeslice, "/aicperbetasum_", betamval, ".csv"), row.names = FALSE)
    
    betaavglst <- list.files(paste0("../output/maxent/p2/models/", timeslice, "/"), pattern = "aicperbetasum_.*csv", full.names = TRUE)
    bavg <- lapply(betaavglst, read.csv, header = TRUE)
    bavg <- do.call(rbind, bavg)
    bavg <- subset(bavg, foldno == f+1)
    write.csv(bavg, paste0("../output/maxent/p2/models/", timeslice, "/avgbetalst.csv"), row.names = FALSE)
  }
    
}

#aicperfoldlst <- list.files(paste0("../output/maxent/p2/models/", timeslice), pattern = "fold4.*AICforMaxentReg.csv", full.names = TRUE)
#aicperfold <- lapply(aicperfoldlst, read.csv, header = TRUE)
#aicperfold <- do.call(rbind, aicperfold)





```

#model selection
next step... select the models you want for each year and run those models again (primarily based on lowest AICc but check test AUC and other things too)

KEEP A NOTE OF THESE IN ONENOTE

2003: 0.25
2004: 1.5
2005: 0.75
2006: 2.25


```{r final models plus predict}
modelmonth <- cbind(paste0(mthna, unique(prback$year)))
aicetc <- read.csv("../data/env/AICcCalculations_p2.csv", header = TRUE) #pre-created csv file
unlayer <- read.csv(paste0("../data/env/asc_layerlist", mthna, ".csv"), header = TRUE) #if code doesnt work put back to unlayer_yr
nfolds <- cbind(c(1:5))

for (year in 1:nrow(modelmonth)){
  timeslice <- modelmonth[year] #testing timeslice
  yr <- str_sub(timeslice, -4) #the year being used for testing
  train <- subset(prback, year == yr) #training data is all years but testing
  trainpa <- train$occurrence #creates a vector of presence/background ID (1/0)
  train  <- subset(train, select = -c(year, occurrence)) #don't need year and occurrence for modelling

  
  if (yr == "2003"){
    betaval <- 0.25
  } else if (yr == "2004"){
    betaval <- 1.5
  } else if (yr == "2005"){
    betaval <- 0.75
  } else if (yr == "2006"){
    betaval <- 2.25
  }
  
  mod <- maxent(x = train, p = trainpa, args=c("maximumiterations=5000", "responsecurves=TRUE", "jackknife=TRUE", "threads=8", "addsamplestobackground=TRUE", paste0("replicates=", nrow(nfolds)), "replicatetype=crossvalidate", paste0("betamultiplier=", betaval)), path = paste0("C:/Users/bunny/Documents/GitHub/Chapter_1/output/maxent/p2/models/", timeslice)) #here the full path is needed
    

    #now based on glover-kapfer pdf with excel manual + https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-1171.1... + adaptation for        getting testAUC.. horrible code
   
    maxresults <- read.csv(paste0("../output/maxent/p2/models/", timeslice, "/maxentResults.csv"), header = TRUE)


    #count number of non-zero coefficients, excluding the final four rows
    lambdas_list <- list.files(paste0("../output/maxent/p2/models/", timeslice), pattern = "*lambdas", full.names = TRUE)
    samppred_list <- list.files(paste0("../output/maxent/p2/models/", timeslice), pattern = "*samplePredictions.csv", full.names = TRUE)
   
      for(f in 1:nrow(nfolds)){
      
      modn <- nfolds[f]-1
      lamf <- lambdas_list[f]
      sampf <- samppred_list[f]
      coefile <- read.csv(paste0(lamf), header = FALSE, stringsAsFactors=FALSE)
      coefile <- head(coefile, -4)
      coefile2 <- subset(coefile, V2 != 0)
      nocoeff <- nrow(coefile2)
      bick <- sum(as.numeric(coefile2[, 2])) #dodgy 
      bic <- (bick*log(sum(trainpa == 1)) - 2*11) #dodgy
      #natural log
      samplepred <- read.csv(paste0(sampf), header = TRUE)
      samplepred$nat_log <- log(samplepred$Raw.prediction)
      sumsamplepred <- sum(samplepred$nat_log)
   
      #no presences used in model (less those used for testing)

      notrainingpoints <- maxresults$X.Training.samples[f]
      testauc <- maxresults$Test.AUC[f]
      trainauc <- maxresults$Training.AUC[f]
      modno <- maxresults$Species[f]
    

      aicetc$train_AUC <- trainauc
      aicetc$test_AUC <- testauc
      aicetc$n <- notrainingpoints
      aicetc$coefficients <- nocoeff
      aicetc$sum.of.logs <- sumsamplepred
  
      aicetc$bic <- bic #not sure this is correct...
      aicetc$beta <- betaval
      aicetc$foldno <- modn
      
    aicetc$AIC <- ((2*aicetc$coefficients)-(2*aicetc$sum.of.logs))
    aicetc$AICc <- (-2*(aicetc$sum.of.logs)+((2*aicetc$coefficients)*(aicetc$n/(aicetc$n - aicetc$coefficients-1))))
    
    #calculate minimal predicted area using ecospat
    aicetc$mpa_0.9_cum <- ecospat.mpa(samplepred$Cumulative.prediction, perc = 0.9) #90%
    
    aicetc$AIC <- ((2*aicetc$coefficients)-(2*aicetc$sum.of.logs))
    aicetc$AICc <- (-2*(aicetc$sum.of.logs)+((2*aicetc$coefficients)*(aicetc$n/(aicetc$n - aicetc$coefficients-1))))
    write.csv(aicetc, paste0("../output/maxent/p2/models/", timeslice, "/", betaval, "_", modn, "_", "AICforMaxentReg.csv"), row.names = FALSE)
      }
    

    aicperbetalst <- list.files(paste0("../output/maxent/p2/models/", timeslice, "/"), pattern = paste0("^", betaval, ".*AICforMaxentReg.csv"), full.names = TRUE)
    aicperbeta <- lapply(aicperbetalst, read.csv, header = TRUE)
    aicperbeta <- do.call(rbind, aicperbeta)
    aicbetamean <- colMeans(aicperbeta)
    aicperbeta <- rbind(aicperbeta, aicbetamean)
    aicperbeta$foldno[f+1] <- f+1
    write.csv(aicperbeta, paste0("../output/maxent/p2/models/", timeslice, "/final_model_aicperbetasum_", betaval, ".csv"), row.names = FALSE)
  
    


#aicperfoldlst <- list.files(paste0("../output/maxent/p2/models/", timeslice), pattern = "fold4.*AICforMaxentReg.csv", full.names = TRUE)
#aicperfold <- lapply(aicperfoldlst, read.csv, header = TRUE)
#aicperfold <- do.call(rbind, aicperfold)




    unlayer_yr <- subset(unlayer, ascyears == yr)
    unlayer_yr <- unlayer_yr[-c(1), ]
    unlayer_yr[] <- lapply(unlayer_yr, as.character) #from https://stackoverflow.com/questions/2851015/convert-data-frame-columns-from-factors-to-characters
    stackdeplst <- list()
    stackdep <- subset(unlayer_yr, select = c(stackname, ascdepths))
    stackdeplst[[year]] <- stackdep
    #write.csv(stackdep, paste0("../output/maxent/p2/predictions/stackdepthlist_", yr, ".csv"), row.names = FALSE)
    aea <- raster("../output/env/aea.tif") 

    for (td in 1:nrow(unlayer_yr)){
      temp_depth <- raster(unlayer_yr$temp_depth[td])
      crs(temp_depth) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
      names(temp_depth) <- "temp_depth"
      temp_depth <- projectRaster(temp_depth, aea)
      chl_surface <- raster(unlayer_yr$chl_surface[td])
      crs(chl_surface) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
      names(chl_surface) <- "chl_surface"
      chl_surface <- projectRaster(chl_surface, aea)
      o2_depth <- raster(unlayer_yr$o2_depth[td])
      crs(o2_depth) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
      names(o2_depth) <- "o2_depth"
      o2_depth <- projectRaster(o2_depth, aea)
      salinity_depth <- raster(unlayer_yr$salinity_depth[td])
      crs(salinity_depth) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
      names(salinity_depth) <- "salinity_depth"
      salinity_depth <- projectRaster(salinity_depth, aea)
      nao_sample <- raster(unlayer_yr$nao_sample[td])
      crs(nao_sample) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
      names(nao_sample) <- "nao_sample"
      nao_sample <- projectRaster(nao_sample, aea)
      nao_prev <- raster(unlayer_yr$nao_prev[td])
      crs(nao_prev) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
      names(nao_prev) <- "nao_prev"
      nao_prev <- projectRaster(nao_prev, aea)
      nao_winter <- raster(unlayer_yr$nao_winter[td])
      crs(nao_winter) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
      names(nao_winter) <- "nao_winter"
      nao_winter <- projectRaster(nao_winter, aea)
      amo_sample <- raster(unlayer_yr$amo_sample[td])
      crs(amo_sample) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0" 
      names(amo_sample) <- "amo_sample"
      amo_sample <- projectRaster(amo_sample, aea)
      amo_winter <- raster(unlayer_yr$amo_winter[td])
      crs(amo_winter) <- "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
      names(amo_winter) <- "amo_winter"
      amo_winter <- projectRaster(amo_winter, aea)
      stackn <- unlayer_yr$stackname[td]
      stack <- stack(temp_depth, salinity_depth, o2_depth, chl_surface, nao_sample, nao_prev, nao_winter, amo_sample, amo_winter)
      #stackSave(test,unlayer_yr$stackname[td])
      pred <- predict(mod, stack, file = paste0("../output/maxent/p2/predictions/", timeslice, "/", "cloglog_", stackn, ".asc"), overwrite=TRUE, args='outputformat=cloglog') 
      predcum <- predict(mod, stack, file = paste0("../output/maxent/p2/predictions/", timeslice, "/", "cumu_", stackn, ".asc"), overwrite=TRUE, args=c('outputformat=Cumulative')) 
      for (pf in 1:nrow(nfolds)){
        plotn <- nfolds[pf]-1
        lyr <- nfolds[pf]
        prplot <- plot(pred[[pf]], png(paste0("../output/maxent/p2/predictions/", timeslice, "/", "cloglog_", stackn, "_", plotn, ".png")))
        dev.off() # stops automatic saving of the plot to a png
      }
    }
}

stdep <- do.call(rbind, stackdeplst)
write.csv(stdep, paste0("../output/maxent/p2/predictions/", mthna, "/stack_depth.csv"), row.names = FALSE)
```

#model averageing

ok! now to average results:

- the asc files per depth layer per year


for this bit may need to stack rasters by depth... this will need some working with the name
get unique filenames into a list
ignore the first X characters in the filename (yr_mthna_) and stack based on the last bit of the filename
then do something probably with raster package


NEEDS TO BE DONE FROM HERE!!

```{r avg asc layers cloglog}
asc_list <- list.files(paste0("../output/maxent/p2/predictions/", mthna), pattern = "cloglog*.asc", full.names = TRUE)
depthlayers <- data.frame(cbind(unique(stdep$ascdepths)))
names(depthlayers)[names(depthlayers) == "cbind.unique.stdep.ascdepths.."] <- "depth"
depthlayers$depth <- as.character(depthlayers$depth)

colno <- 1

for (d in 1:nrow(depthlayers)){
  dep <- depthlayers$depth[d]
  for (a in 1:length(asc_list)){
    asc <- asc_list[a]
    if (grepl(dep, asc)){
      depthlayers[d , paste0("a", colno)] <- asc
        colno <- colno+1
    }
  }
}
write.csv(depthlayers, (paste0("../output/maxent/p2/predictions/", mthna, "/cloglog_asclayerbydepth.csv")), row.names = TRUE)


for (x in 1:nrow(depthlayers)){
  asclay <- depthlayers[x, ]
  asclay <- asclay[!is.na(asclay)]
  asclay <- as.data.frame(rbind(asclay))
  asclay$V1 <- as.character(asclay$V1) #v1 corresponds to depth
  asclay$V2 <- as.character(asclay$V2) #v2 to end corresponds to year
  asclay$V3 <- as.character(asclay$V3)
  asclay$V4 <- as.character(asclay$V4)
  asclay$V5 <- as.character(asclay$V5)
  asclay$V6 <- as.character(asclay$V6)
  asclay$V7 <- as.character(asclay$V7)
  asclay$V8 <- as.character(asclay$V8)
  asclay$V9 <- as.character(asclay$V9)
  asclay$V10 <- as.character(asclay$V10)
  asclay$V11 <- as.character(asclay$V11)
  asclay$V12 <- as.character(asclay$V12)
  asclay$V13 <- as.character(asclay$V13)
  asclay$V14 <- as.character(asclay$V14)
  asclay$V15 <- as.character(asclay$V15)  
  
  asclaydep <- asclay$V1
  
  asc1998 <- raster(asclay$V2)
  asc1999 <- raster(asclay$V3)
  asc2000 <- raster(asclay$V4)
  asc2001 <- raster(asclay$V5)
  asc2002 <- raster(asclay$V6)
  asc2003 <- raster(asclay$V7)
  asc2004 <- raster(asclay$V8)
  asc2005 <- raster(asclay$V9)
  asc2006 <- raster(asclay$V10)
  asc2007 <- raster(asclay$V11)
  asc2008 <- raster(asclay$V12)
  asc2009 <- raster(asclay$V13)
  asc2010 <- raster(asclay$V14)
  asc2011 <- raster(asclay$V15)

  stkasc <- stack(asc1998, asc1999, asc2000, asc2001, asc2002, asc2003, asc2004, asc2005, asc2006, asc2007, asc2008, asc2009, asc2010, asc2011)
  avg <- calc(stkasc, fun = mean)
  
  writeRaster(avg, (paste0("../output/maxent/p2/predictions/", mthna, "/avg_", asclaydep, ".asc")), overwrite=TRUE, args='outputformat=cloglog')
  avgplot <- plot(avg, png(paste0("../output/maxent/p2/predictions/", mthna, "/avg_", asclaydep, ".png"))) 
  dev.off()
}


ascdepth_list <- list.files(paste0("../output/maxent/p2/predictions/", mthna), glob2rx(pattern = "avg_*.asc"), full.names = TRUE)


clog_asc120  <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_120.asc"))
clog_asc41.18 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_41.18.asc"))
clog_asc1.55586 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_1.55586.asc"))
clog_asc1045.85 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_1045.85.asc"))
clog_asc108.03 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_108.03.asc"))
clog_asc11.7737 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_11.7737.asc"))
clog_asc1151.99 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_1151.99.asc"))
clog_asc1265.86 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_1265.86.asc"))
clog_asc13.991 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_13.991.asc"))
clog_asc133.076 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_133.076.asc"))
clog_asc1387.38 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_1387.38.asc"))
clog_asc147.406 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_147.406.asc"))
clog_asc16.5253 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_16.5253.asc"))
clog_asc163.165 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_163.165.asc"))
clog_asc180.55 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_180.55.asc"))
clog_asc19.4298 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_19.4298.asc"))
clog_asc199.79 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_199.79.asc"))
clog_asc2.66768 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_2.66768.asc"))
clog_asc22.7576 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_22.7576.asc"))
clog_asc221.141 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_221.141.asc"))
clog_asc244.891 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_244.891.asc"))
clog_asc26.5583 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_26.5583.asc"))
clog_asc271.356 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_271.356.asc"))
clog_asc3.85628 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_3.85628.asc"))
clog_asc30.8746 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_30.8746.asc"))
clog_asc300.888 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_300.888.asc"))
clog_asc333.863 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_333.863.asc"))
clog_asc35.7402 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_35.7402.asc"))
clog_asc370.689 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_370.689.asc"))
clog_asc411.794 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_411.794.asc"))
clog_asc457.626 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_457.626.asc"))
clog_asc47.2119 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_47.2119.asc"))
clog_asc5.14036 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_5.14036.asc"))
clog_asc508.64 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_508.64.asc"))
clog_asc53.8506 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_53.8506.asc"))
clog_asc565.292 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_565.292.asc"))
clog_asc6.54303 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_6.54303.asc"))
clog_asc61.1128 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_61.1128.asc"))
clog_asc628.026 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_628.026.asc"))
clog_asc69.0217 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_69.0217.asc"))
clog_asc697.259 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_697.259.asc"))
clog_asc77.6112 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_77.6112.asc"))
clog_asc773.368 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_773.368.asc"))
clog_asc8.09252 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_8.09252.asc"))
clog_asc856.679 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_856.679.asc"))
clog_asc86.9294 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_86.9294.asc"))
clog_asc9.82275 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_9.82275.asc"))
clog_asc947.448 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_947.448.asc"))
clog_asc97.0413 <- raster(paste0("../output/maxent/p2/predictions/", mthna, "/avg_97.0413.asc"))

stackavg <- stack(clog_asc120, clog_asc41.18, clog_asc1.55586, clog_asc1045.85, clog_asc108.03, clog_asc11.7737, clog_asc1151.99, clog_asc1265.86, clog_asc13.991, clog_asc133.076, clog_asc1387.38, clog_asc147.406, clog_asc16.5253, clog_asc163.165, clog_asc180.55, clog_asc19.4298, clog_asc199.79, clog_asc2.66768, clog_asc22.7576, clog_asc221.141, clog_asc244.891, clog_asc26.5583, clog_asc271.356, clog_asc3.85628, clog_asc30.8746, clog_asc300.888, clog_asc333.863, clog_asc35.7402, clog_asc370.689, clog_asc411.794, clog_asc457.626, clog_asc47.2119, clog_asc5.14036, clog_asc508.64, clog_asc53.8506, clog_asc565.292, clog_asc6.54303, clog_asc61.1128, clog_asc628.026, clog_asc69.0217, clog_asc697.259, clog_asc77.6112, clog_asc773.368, clog_asc8.09252, clog_asc856.679, clog_asc86.9294, clog_asc9.82275, clog_asc947.448, clog_asc97.0413)

stsum <- sum(stackavg, na.rm = TRUE)
  writeRaster(stsum, (paste0("../output/maxent/p2/predictions/", mthna, "/clog_sum_", mthna, ".asc")), overwrite=TRUE)
  sumplot <- plot(stsum, png(paste0("../output/maxent/p2/predictions/", mthna, "/clog_sum_", mthna, ".png"))) 
  dev.off()
stavg <- calc(stackavg, fun = mean, na.rm = TRUE)
  writeRaster(stavg, (paste0("../output/maxent/p2/predictions/", mthna, "/clog_avg_", mthna, ".asc")), overwrite=TRUE)
  avgplot <- plot(stavg, png(paste0("../output/maxent/p2/predictions/", mthna, "/clog_avg_", mthna, ".png"))) 
  dev.off()

```









asc120
asc41.18
asc1.55586
asc1045.85
asc108.03
asc11.7737
asc1151.99
asc1265.86
asc13.991
asc133.076
asc1387.38
asc147.406
asc16.5253
asc163.165
asc180.55
asc19.4298
asc199.79
asc2.66768
asc22.7576
asc221.141
asc244.891
asc26.5583
asc271.356
asc3.85628
asc30.8746
asc300.888
asc333.863
asc35.7402
asc370.689
asc411.794
asc457.626
asc47.2119
asc5.14036
asc508.64
asc53.8506
asc565.292
asc6.54303
asc61.1128
asc628.026
asc69.0217
asc697.259
asc77.6112
asc773.368
asc8.09252
asc856.679
asc86.9294
asc9.82275
asc947.448
asc97.0413


