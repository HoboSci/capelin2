---
title: "timesliced_random_cell_selection"
author: "Samantha Andrews"
output: html_notebook
---

# Overview
Preperation of the capelin (*Mallotus villosus*) data obtained from [OBIS](www.iobis.org). This database shows the global occurance of capelin. 

A note to anyone who might happen to stumble across this... I am a beginner in R and have had no exposure to similar languages. I don't know what I'm doing. The code herein is unlikely to be elegant and there area probably more efficient ways of running the code.

Built with 'r getRversion()'.

# Package dependencies
You can install and load them using the following code which uses a function called [ipak](https://gist.github.com/stevenworthington/3178163). Note this function checks to see if the packages are installed first before loading.


New package attempt to do this....
old code 
packages <- c("raster", "dplyr", "data.table", "sp", "adehabitatHR", "maptools")
source("../src/ipak.R")
ipak(packages)


```{r pre-install & load packages, include=FALSE}
install.packages("pacman")
library(pacman)
p_load(raster, dplyr, data.table, sp, adehabitatHR, rgdal, rworldmap)
```

# the show
you have a unique_cell raster already (glo_unique_cell) (output/env/glo_unique_cell.tif)

```{r}
glo_unique_cell <- raster("../output/env/glo_unique_cell.tif") #this is loading the cell layer that was created and saved as a tif
plot(glo_unique_cell)
```

as a reminder, how many cells are there?
```{r}
glo_unique_cell
```

7750

ok now you want to generate a centroid for each cell... (a point in the center of each cell)
use the raster layer to 1) transfer the raster into a dataframe, with the values (cell numbers) linked to each centroid with xy coordinates (note as the raster is an aea projection, the xy will be in meters)
```{r}
unique_cell_centroid <- as.data.frame(glo_unique_cell, row.names = NULL, optional = FALSE, xy = TRUE, centroids = TRUE, na.rm = TRUE)
names(unique_cell_centroid)[names(unique_cell_centroid)=="layer.1"] <- "cell_id" #just renames the output
names(unique_cell_centroid)[names(unique_cell_centroid)=="x"] <- "longitude" #just renames the output
names(unique_cell_centroid)[names(unique_cell_centroid)=="y"] <- "latitude" #just renames the output
write.csv(unique_cell_centroid, "../output/env/unique_cell_centroid.csv")
plot(glo_unique_cell)
points(unique_cell_centroid$longitude, unique_cell_centroid$latitude, pch = 10, col = "black")
write.csv(unique_cell_centroid, "../output/env/unique_cell_points.csv", row.names = FALSE)
```
well... the plot doesn't tell me much... but i definately have points!

The area covered by these points are huge -much larger than the distribution of my presence points. Ideally I need to constrain the area where the background points are selected from to match the entire range of the species. The most common method for this is to use a minimum convex hull to estimate the species range, and then use that area to constrain the background point selection.
Note I am a little worried that I may loose some of the edge data as they are centroids of cells... 

```{r}
mcpbase <- read.csv("../output/bio/data_aea_cell_amo_nao_bins_env_depth_cellobno_env.csv", header = TRUE)
mcpbase_xy <- mcpbase[ , c("decimalLongitude.1", "decimalLatitude.1")] # This is to tell R where the coordinates are (in column 18 and 19). Note that the column order needs to be longitude, latitude
mcpbase_sp <- SpatialPointsDataFrame(coords = mcpbase_xy, data = mcpbase, proj4string = CRS("+proj=aea +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-60 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")) # The CRS is used here is for the albers equal area projection.
mcp_capelin_100 <- mcp(mcpbase_sp[ , 12], percent = 100) #12 is the column with the species name in (the id for the species)
plot(mcp_capelin_100)
```

oh tres exciting. Let's overlay the occurrence points on that

```{r}
plot(mcp_capelin_100)
points(mcpbase_sp$decimalLongitude.1, mcpbase_sp$decimalLatitude.1)
```

ok yea, i guess.... still wondering about those edge areas....anyway save the hull as a polygon

```{r}
writeOGR(mcp_capelin_100, dsn = '.', layer = '../output/bio/mcp_capelin_100_poly', driver = "ESRI Shapefile")
```

now need to select points within a polygon....
```{r}
unique_cell_centroid <- read.csv("../output/env/unique_cell_centroid.csv",header = TRUE)
unique_cell_xy <- unique_cell_centroid[ , c("longitude", "latitude")] # This is to tell R where the coordinates are (in column 18 and 19). Note that the column order needs to be longitude, latitude
unique_c_sp <- SpatialPointsDataFrame(coords = unique_cell_xy, data = unique_cell_centroid, proj4string = CRS("+proj=aea +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-60 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")) # The CRS is used here is for the albers equal area projection.
plot(mcp_capelin_100)
points(unique_c_sp, pch = "red")
```

```{r}
mcp_unique_cell <- unique_c_sp[mcp_capelin_100, ] #this is just subsetting 
plot(mcp_capelin_100)
points(mcp_unique_cell)
```
ok now convert mcp_unique_cell back to a dataframe for further processing, and save the clipped unique cells to a .csv
```{r}
mcp_unique_cell <- as.data.frame(mcp_unique_cell)
write.csv(mcp_unique_cell, "../output/env/unique_cell_centroid_mcp")
```



ok so now I have a base list to work off... This gives you a dataset of xy points, but you need xyz (where z is based on the depth layers in the environmental data)
```{r}
unique_cell_xyz <- mcp_unique_cell
unique_cell_xyz$depthlayerno <- NA
unique_cell_xyz <- unique_cell_xyz[rep(seq_len(nrow(unique_cell_xyz)), each=50),] #this generates 50 repetitions of each row
unique_cell_xyz$depthlayerno <- rep(1:50) #assign 1:50 on a repeating cycle to the depth column. 50 is chosen because there are 50 depth layers
unique_cell_xyz$id_depth <- paste(unique_cell_xyz$glo_unique_cell, unique_cell_xyz$depthlayerno, sep="") #create a new column that is a unique glo_unique_cell & depth ID. This is to help remove cells later and randomise
```



What I want to do is for each time slice, create a list of all cells (xyz), then remove the ones where observations occurred and create a .csv with the name of the level being worked on

Split the data_aea into timeslices (year/month).
All you need is the year, month, cell_id, and depthlayerno
```{r}
data_aea <- mcpbase
cell_yymm <- subset(data_aea, select= c(cell_id, depthlayerno, year, month)) #create a new df with just cell_id, yr, month
cell_yymm$id_depth <- paste(cell_yymm$cell_id, cell_yymm$depthlayerno, sep="") #create a new column that is a unique cell_id & depth ID. This is to help remove cells later and randomise
split_cell_yymm <- split(cell_yymm, list(cell_yymm$year, cell_yymm$month), drop = TRUE)
```

Here is a loop to create a copy of the unique_cell_centroid dataframe, and give it the same name as the splitobs_cell_yymm level
.csv naming code courtesy of [nadizan/stack exchange](https://stackoverflow.com/questions/53045158/r-naming-csv-with-a-list-level-name-in-a-loop?noredirect=1#comment92990285_53045158)


```{r timesliced unique unsampled}
no_timeslices <- length(split_cell_yymm) # how many levels (time slices) are in the list
bckoutput <- "../output/bio/background/raw/" #where the files are to be saved to
for (i in 1:no_timeslices){
  timeslice <- split_cell_yymm[[i]]
  timeslice <- as.data.frame(timeslice)
  time_cell <- unique_cell_xyz
  time_cell <- time_cell[!time_cell$id_depth %in% timeslice$id_depth, , drop = FALSE]
  write.csv(time_cell, paste0(bckoutput, names(lapply(split_cell_yymm, names))[i], ".csv"), row.names = FALSE) 
}
```

Ok so now I have a bunch of .csv files containing cells where no observations occurred for a given timeslice on xyz coordinates.
Next step is to randomly select cells in each timeslice. Make sure you generate new files for each...

load the .csvs... [code](https://www.reed.edu/data-at-reed/software/R/reading_and_writing.html)

```{r}
bckrnd_folder <- "../output/bio/background/raw/"      # path to folder that holds multiple .csv files
bckrnd_list <- list.files(path = bckrnd_folder, pattern="*.csv") # create list of all .csv files in folder
randbckoutput <- "../output/bio/background/rand/" #where the files are to be saved to
# read in each .csv file in file_list and create a data frame with the same name as the .csv file
for (i in 1:length(bckrnd_list)){
  sel <- read.csv(paste(bckrnd_folder, bckrnd_list[i], sep=''))
  sel <- sample_n(sel, 10000) #where 10000 = number of rows to sample (large sample as per maxent)
  write.csv(sel, file = paste0(randbckoutput, bckrnd_list[i]), row.names = FALSE) #dont need to add .csv as it's already in the filename
}
head(sel)  
```

ok now i have a ton of files for random background points broken into timeslices. I'd like to add a colum for year and one for month to each of these datasets. They should be populated based on the name of the file (e.g. 1998.4.csv would have year = 1998 and month = 4)

```{r}
rand_folder <- "../output/bio/background/rand/"      # path to folder that holds multiple .csv files
rand_list <- list.files(path = rand_folder, pattern="*.csv") # create list of all .csv files in folder
randbckoutput <- "../output/bio/background/rand/" #where the files are to be saved to
for(i in 1:length(rand_list)){
  randtemp <- read.csv(paste(rand_folder, rand_list[i], sep=''))
  randtemp$year <- substr(rand_list[i], 1, 4) #the year is characters 1-4 in the filename
  randtemp$month <- substr(rand_list[i], 6, 7) #the month is characters 6 & 7 in the filename (except where the month is a single digit)
  write.csv(randtemp, file = paste0(randbckoutput, bckrnd_list[i]), row.names = FALSE) #dont need to add .csv as it's already in the filename
}
head(randtemp)
```
note that in the output temp file in R shows a . after the month if that month is a single digit. The .csv outputs don't 

# populating the background points data with... data

ok now you can start to populate the rest of the background point information. The background points file should mirror as much as possible the occurence file. Maybe it would be best to merge the files together, and them split them again later? would save looping everything to repeat the process through multiple files...

```{r}
filenames  <- list.files(path = rand_folder, pattern="*.csv", full.names = TRUE) 
background_all <- rbindlist(lapply(filenames,fread, fill = TRUE))
write.csv(background_all, "../output/bio/background_all.csv", row.names = FALSE)
head(background_all)
```

check for unique years
```{r}
back_yr_uniq <- unique(background_all$year)
back_yr_uniq
```
check for unique months
```{r}
back_mt_uniq <- unique(background_all$month)
back_mt_uniq
```

check number of observations 
```{r}
back_rowcount <- nrow(background_all)
back_rowcount
```



get the columns from the occurence data so you know what to add
```{r}
occ_column <- colnames(data_aea)
occ_column
```

ok so have cell_id, year, month, decimalLongitude.1(meters - in background_all its longitude), decimalLatitude.1 (meters - in background_all its latitude), depth_layer_no.

Easy first one - add an id based on id_depth
```{r}
background_all$id <- background_all$id_depth
write.csv(background_all, "../output/bio/background_all.csv", row.names = FALSE)
head(background_all)
```

ok lets's get the real decimalLatitude and decimalLongitude.
Will need to plot all point, extract the lonlat coordinates

1) load the file as a spatialdatapointframe. give it the aea coordinate

```{r}
background_all <- as.data.frame(background_all)
xy <- background_all[ ,c(2,3)] #lon lat in col 1 and 2
sp_background <- SpatialPointsDataFrame(coords = xy, data = background_all, proj4string = CRS("+proj=aea +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-60 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"))
head(sp_background)
```

```{r}
plot(sp_background)
```

2) reproject to a wgs (use an existing file as a mask)

```{r}
sp_background <- spTransform(sp_background, CRS = "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")
plot(sp_background)
```
3) extract lonlat, add to dataframe

```{r}
sp_background <- data.frame(sp_background, long=coordinates(sp_background)[,1], lat=coordinates(sp_background)[,2])
head(sp_background)
```

remove existing decimlalLongitude and decimlalLongitude, and long and lat, and optional (duplicate columns) and change name of longitude.2 & latitude.2 columns to decimalLongitude and decimalLatitude respectively
```{r}
sp_background <- as.data.frame(sp_background) #need to go back to a 'normal' dataframe to change col names
head(sp_background) 
sp_background <- subset(sp_background, select = -c(decimalLongitude, decimalLatitude, long, lat, optional))
colnames(sp_background)[colnames(sp_background)=="longitude.2"] <- "decimalLongitude"
colnames(sp_background)[colnames(sp_background)=="latitude.2"] <- "decimalLatitude"
head(sp_background)
```


save as csv
```{r}
write.csv(sp_background, file = "../output/bio/background_all_lonlat.csv", row.names = FALSE)
```


# Add NAFO Region background point is in
NAFO Zones shapefile obtained from [NAFO](https://www.nafo.int/Data/GIS)

```{r NAFO shapefile extraction}
coordinates(sp_background) <- c("decimalLongitude", "decimalLatitude") 
nafo_zones <- readOGR(dsn=path.expand("../data/bio/nafo_zones"), layer = "nafo_zones_wgs84") #this loads the shapefile
proj4string(sp_background) <- proj4string(nafo_zones) #tells R that the occurrence data is the same projection as the shapefile
sp_background$nafo_zone <- over(sp_background, nafo_zones)$ZONE #ZONE is where the zone data is held in teh shapefiles' attributes
head(sp_background) #just to check it worked
```

Great. Lets see if any are missing a NAFO Zone. But first, write to a csv and reload as the data is now some wierd 'spatial points dataframe'...
```{r}
write.csv(sp_background, "../output/bio/background_all_lonlat_nafo.csv", row.names=FALSE)
sp_background <- as.data.frame(sp_background) 
bc_nafo_na <- subset(sp_background, is.na(sp_background$nafo_zone))
bc_nafo_na
```
ok lots don't have a NAFO region attached. This is probably the Hudson Bay issue again...

Uh oh there is a lot. A bet a bunch of them are in the Hudson Strait (which is outside the NAFO Zone).Map the nafo_na points...
```{r}
map2 <- getMap(resolution = "low") #creates an object called map at low resoultion
plot(map2, xlim = c(-65, -40), ylim =c(30, 70), asp = 1, main = "Background missing a NAFO Zone", col = "cornsilk") #the x and y lim are the long-lat bounds of the map
points(bc_nafo_na$decimalLongitude, bc_nafo_na$decimalLatitude, col = "red") #this adds points to the mapet", xlab = "Longitude", ylab = "Latitude")
dev.copy(png, "../output/bio/background_missing_nafo.png") #this prints a png of the plot
dev.off() #this turns off the print comman
```

Indeed the many of the missing background points are in the hudson strait. Others are near the coast and thus probably just call outside the NAFO shapefiles (remember the smapefiles are not all super-perfectly fitting).
Think I will take this to arcgis for fixing....

basically what I did was use the selection -> select by attributes tool to find the points where NAFO_region = NA and then manually recode them (using the field calculator), and exported as a .csv.

just double-check they are all gone
```{r}
background_all <- read.csv("../output/bio/background_all_lonlat_nafo2.csv", header = TRUE)
bc_nafo2_na <- subset(background_all, is.na(background_all$nafo_zone))
bc_nafo2_na
```
great none!


```{r}
head(background_all)
```
ok so some more crap has been added - the year and month both have _ appended.... change this. Also change 
- glo_unique_cell to cell_id
- longitude to longitude_meters
- latitude to latitude_meters

and remove
- optional
- x.1
- x
- id_depth

```{r}
names(background_all)[names(background_all)=="year_"] <- "year"
names(background_all)[names(background_all)=="month_"] <- "month"
names(background_all)[names(background_all)=="glo_unique_cell"] <- "cell_id"
names(background_all)[names(background_all)=="longitude"] <- "longitude_meters"
names(background_all)[names(background_all)=="latitude"] <- "latitude_meters"
background_all <- subset(background_all, select = -c(optional, X.1, X, id_depth))
head(background_all)
```


#original scientific name

create a column called "originalscientificname" and fill it with "Mallotus villosus ab"

```{r}
background_all$originalscientificname <- "Mallotus villosus ab"
write.csv(background_all, "../output/bio/background_all_lonlat_nafo2_sciname.csv")
head(background_all)
```

# Add AMO Data
The data has been prepared as per the environmental_data_preperation.Rmd, and biological_data_prep.Rmd

So you want..
- AMO value at sampling month/year
- AMO value at previous sampling month/year
- AMO phase at previous winter (because the winter values are thought to be a major driver of ocean conditions in the following spring, summer, and autumn)

Load the prepared amo_prev.csv file with has the amo at different year/month steps
```{r}
amo_prev <- read.csv("../output/env/amo_prev.csv", header = TRUE)
head(amo_prev)
```

now match the year and the month in the two dataframes, and populate the background points with amo_sample
```{r}
background_all <- merge(x = background_all, y = amo_prev[ , c("year", "month", "amo_sample")], by = c("year", "month"), all.x = TRUE)
head(background_all)
```

ok next - AMO value at previous sampling month/year 
```{r}
background_all <- merge(x = background_all, y = amo_prev[ , c("year", "month", "amo_prev")], by = c("year", "month"), all.x = TRUE)
head(background_all)
```

and now - AMO phase at previous winter (because the winter values are thought to be a major driver of ocean conditions in the following spring, summer, and autumn) (note they are in a different .csv)

```{r}
amo_winter <- read.csv("../output/env/amo_winter.csv")
background_all <- merge(x = background_all, y = amo_winter[ , c("year", "WinterAvg.Shifted")], by = c("year"), all.x = TRUE)
colnames(background_all)[colnames(background_all)=="WinterAvg.Shifted"] <- "amo_winter" #change the colname
head(background_all)
```

ok and write a csv
```{r}
write.csv(data_aea,"../output/bio/background_all_lonlat_nafo2_sciname_amo.csv", row.names = FALSE)
```


# NAO values
The data has been prepared as per the environmental_data_preperation.Rmd and biological_data_prep.Rmd

So you want..
- NAO value at sampling month/year
- NAO value at previous sampling month/year
- NAO phase at previous winter (because the winter values are thought to be a major driver of ocean conditions in the following spring, summer, and autumn)

Load the prepared nao_prev.csv file with has the amo at different year/month steps
```{r}
nao_prev <- read.csv("../output/env/nao_prev.csv", header = TRUE)
head(nao_prev)
```

now match the year and the month in the two dataframes, and populate the background points with nao_sample

```{r}
background_all <- merge(x = background_all, y = nao_prev[ , c("year", "month", "nao_sample")], by = c("year", "month"), all.x = TRUE)
background_all
```

ok next - NAO value at previous sampling month/year
```{r}
background_all <- merge(x = background_all, y = nao_prev[ , c("year", "month", "nao_prev")], by = c("year", "month"), all.x = TRUE)
background_all
```

and now - NAO phase at previous winter (because the winter values are thought to be a major driver of ocean conditions in the following spring, summer, and autumn). The data is in a different .CSV

```{r}
nao_winter <- read.csv("../output/env/nao_winter.csv")
background_all <- merge(x = background_all, y = nao_winter[ , c("year", "WinterAvg.Shifted")], by = c("year"), all.x = TRUE)
colnames(background_all)[colnames(background_all)=="WinterAvg.Shifted"] <- "nao_winter" #change the colname
background_all
```

and write .csv
```{r}
write.csv(background_all,"../output/bio/background_all_lonlat_nafo2_sciname_amo_nao.csv", row.names = FALSE)
```

still need
-bottom_depth
-depth_layer
-total_cell_obs
-yymm_cell_obs

#depth layer


