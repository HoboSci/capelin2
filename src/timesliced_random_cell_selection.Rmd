---
title: "timesliced_random_cell_selection"
author: "Samantha Andrews"
output: html_notebook
---

# Overview
Preperation of the capelin (*Mallotus villosus*) data obtained from [OBIS](www.iobis.org). This database shows the global occurance of capelin. 

A note to anyone who might happen to stumble across this... I am a beginner in R and have had no exposure to similar languages. I don't know what I'm doing. The code herein is unlikely to be elegant and there area probably more efficient ways of running the code.

Built with 'r getRversion()'.

# Package dependencies
You can install and load them using the following code which uses a function called [ipak](https://gist.github.com/stevenworthington/3178163). Note this function checks to see if the packages are installed first before loading.


New package attempt to do this....
old code 
packages <- c("raster", "dplyr", "data.table", "sp", "adehabitatHR", "maptools")
source("../src/ipak.R")
ipak(packages)

```{r pre-install & load packages, include=FALSE}
if (!require("pacman")) install.packages("pacman")
p_load(raster, dplyr, data.table, sp, adehabitatHR, rgdal, rworldmap, ncdf4, marmap, plyr, weathermetrics)
```

# the show
you have a unique_cell raster already (glo_unique_cell) (output/env/glo_unique_cell.tif)

```{r}
glo_unique_cell <- raster("../output/env/glo_unique_cell.tif") #this is loading the cell layer that was created and saved as a tif
plot(glo_unique_cell)
```

as a reminder, how many cells are there?
```{r}
glo_unique_cell
```

7750

ok now you want to generate a centroid for each cell... (a point in the center of each cell)
use the raster layer to 1) transfer the raster into a dataframe, with the values (cell numbers) linked to each centroid with xy coordinates (note as the raster is an aea projection, the xy will be in meters)
```{r}
unique_cell_centroid <- as.data.frame(glo_unique_cell, row.names = NULL, optional = FALSE, xy = TRUE, centroids = TRUE, na.rm = TRUE)
names(unique_cell_centroid)[names(unique_cell_centroid)=="layer.1"] <- "cell_id" #just renames the output
names(unique_cell_centroid)[names(unique_cell_centroid)=="x"] <- "longitude" #just renames the output
names(unique_cell_centroid)[names(unique_cell_centroid)=="y"] <- "latitude" #just renames the output
write.csv(unique_cell_centroid, "../output/env/unique_cell_centroid.csv")
plot(glo_unique_cell)
points(unique_cell_centroid$longitude, unique_cell_centroid$latitude, pch = 10, col = "black")
write.csv(unique_cell_centroid, "../output/env/unique_cell_points.csv", row.names = FALSE)
```
well... the plot doesn't tell me much... but i definately have points!

The area covered by these points are huge -much larger than the distribution of my presence points. Ideally I need to constrain the area where the background points are selected from to match the entire range of the species. The most common method for this is to use a minimum convex hull to estimate the species range, and then use that area to constrain the background point selection.
Note I am a little worried that I may loose some of the edge data as they are centroids of cells... 

```{r}
mcpbase <- read.csv("../output/bio/data_aea_cell_amo_nao_bins_env_depth_cellobno_env.csv", header = TRUE)
mcpbase_xy <- mcpbase[ , c("decimalLongitude.1", "decimalLatitude.1")] # This is to tell R where the coordinates are (in column 18 and 19). Note that the column order needs to be longitude, latitude
mcpbase_sp <- SpatialPointsDataFrame(coords = mcpbase_xy, data = mcpbase, proj4string = CRS("+proj=aea +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-60 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")) # The CRS is used here is for the albers equal area projection.
mcp_capelin_100 <- mcp(mcpbase_sp[ , 12], percent = 100) #12 is the column with the species name in (the id for the species)
plot(mcp_capelin_100)
```

oh tres exciting. Let's overlay the occurrence points on that

```{r}
plot(mcp_capelin_100)
points(mcpbase_sp$decimalLongitude.1, mcpbase_sp$decimalLatitude.1)
```

ok yea, i guess.... still wondering about those edge areas....anyway save the hull as a polygon

```{r}
writeOGR(mcp_capelin_100, dsn = '.', layer = '../output/bio/mcp_capelin_100_poly', driver = "ESRI Shapefile")
```

now need to select points within a polygon....
```{r}
unique_cell_centroid <- read.csv("../output/env/unique_cell_centroid.csv",header = TRUE)
unique_cell_xy <- unique_cell_centroid[ , c("longitude", "latitude")] # This is to tell R where the coordinates are (in column 18 and 19). Note that the column order needs to be longitude, latitude
unique_c_sp <- SpatialPointsDataFrame(coords = unique_cell_xy, data = unique_cell_centroid, proj4string = CRS("+proj=aea +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-60 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")) # The CRS is used here is for the albers equal area projection.
plot(mcp_capelin_100)
points(unique_c_sp, pch = "red")
```

```{r}
mcp_unique_cell <- unique_c_sp[mcp_capelin_100, ] #this is just subsetting 
plot(mcp_capelin_100)
points(mcp_unique_cell)
```
ok now save the clipped unique cells to a .csv
```{r}
write.csv(mcp_unique_cell, "../output/env/unique_cell_centroid_mcp")
```

#decimalLat and Lon
ok lets's get the real decimalLatitude and decimalLongitude.
Will need to plot all point, extract the lonlat coordinates

reproject to a wgs (use an existing file as a mask)

```{r}
sp_mcp_unique_cell <- spTransform(mcp_unique_cell, CRS = "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")
plot(sp_mcp_unique_cell)
```
extract lonlat, add to dataframe

```{r}
sp_mcp_unique_cell <- data.frame(sp_mcp_unique_cell, long=coordinates(sp_mcp_unique_cell)[,1], lat=coordinates(sp_mcp_unique_cell)[,2])
head(sp_mcp_unique_cell)
```

remove existing long, lat, optional, longitude.1, latitude.1 (duplicate columns) and change name of longitude.2 & latitude.2 columns to decimalLongitude and decimalLatitude respectively, and longitude and latitude to longitude_meters and latitude_meters ande glo_unique_cell to cell_id
```{r}
sp_mcp_unique_cell <- as.data.frame(sp_mcp_unique_cell) #need to go back to a 'normal' dataframe to change col names
head(sp_mcp_unique_cell) 
sp_mcp_unique_cell <- subset(sp_mcp_unique_cell, select = -c(long, lat, optional, longitude.1, latitude.1))
colnames(sp_mcp_unique_cell)[colnames(sp_mcp_unique_cell)=="longitude.2"] <- "decimalLongitude"
colnames(sp_mcp_unique_cell)[colnames(sp_mcp_unique_cell)=="latitude.2"] <- "decimalLatitude"
colnames(sp_mcp_unique_cell)[colnames(sp_mcp_unique_cell)=="longitude"] <- "longitude_meters"
colnames(sp_mcp_unique_cell)[colnames(sp_mcp_unique_cell)=="latitude"] <- "latitude_meters"
colnames(sp_mcp_unique_cell)[colnames(sp_mcp_unique_cell)=="glo_unique_cell"] <- "cell_id"
head(sp_mcp_unique_cell)
```


save as csv
```{r}
write.csv(sp_background, file = "../output/bio/unique_cell_centroid_lonlat.csv", row.names = FALSE)
```


# Add NAFO Region background points in
NAFO Zones shapefile obtained from [NAFO](https://www.nafo.int/Data/GIS)

```{r NAFO shapefile extraction}
coordinates(sp_mcp_unique_cell) <- c("decimalLongitude", "decimalLatitude") 
nafo_zones <- readOGR(dsn=path.expand("../data/bio/nafo_zones"), layer = "nafo_zones_wgs84") #this loads the shapefile
proj4string(sp_mcp_unique_cell) <- proj4string(nafo_zones) #tells R that the occurrence data is the same projection as the shapefile
sp_mcp_unique_cell$nafo_zone <- over(sp_mcp_unique_cell, nafo_zones)$ZONE #ZONE is where the zone data is held in teh shapefiles' attributes
head(sp_mcp_unique_cell) #just to check it worked
```

Great. Lets see if any are missing a NAFO Zone. But first, write to a csv and reload as the data is now some wierd 'spatial points dataframe'...
```{r}
write.csv(sp_mcp_unique_cell, "../output/bio/unique_cell_centroid_lonlat_nafo.csv", row.names=FALSE)
sp_mcp_unique_cell <- as.data.frame(sp_mcp_unique_cell) 
bc_nafo_na <- subset(sp_mcp_unique_cell, is.na(sp_mcp_unique_cell$nafo_zone))
bc_nafo_na
```

ok lots don't have a NAFO region attached. This is probably the Hudson Bay issue again...

Uh oh there is a lot. A bet a bunch of them are in the Hudson Strait (which is outside the NAFO Zone).Map the nafo_na points...
```{r}
map2 <- getMap(resolution = "low") #creates an object called map at low resoultion
plot(map2, xlim = c(-65, -40), ylim =c(30, 70), asp = 1, main = "Background missing a NAFO Zone", col = "cornsilk") #the x and y lim are the long-lat bounds of the map
points(bc_nafo_na$decimalLongitude, bc_nafo_na$decimalLatitude, col = "red") #this adds points to the mapet", xlab = "Longitude", ylab = "Latitude")
dev.copy(png, "../output/bio/background_missing_nafo.png") #this prints a png of the plot
dev.off() #this turns off the print comman
```

Indeed the many of the missing background points are in the hudson strait. Others are near the coast and thus probably just call outside the NAFO shapefiles (remember the smapefiles are not all super-perfectly fitting).
Think I will take this to arcgis for fixing....

basically what I did was use the selection -> select by attributes tool to find the points where NAFO_region = NA and then manually recode them (using the field calculator), and exported as a .csv.

just double-check they are all gone
```{r}
mcp_unique_cell <- read.csv("../output/env/unique_cell_centroid_lonlat_nafo2.csv", header = TRUE)
bc_nafo2_na <- subset(mcp_unique_cell, is.na(mcp_unique_cell$nafo_zone))
bc_nafo2_na
```

# random depth


ok so now I have a base list to work off... This gives you a dataset of xy points, but you need xyz (where z is based on the depth layers in the environmental data). This should be constrained by the presence depth range
```{r}
max_obs_depth <- max(mcpbase$depthlayerno, na.rm = TRUE) #max is 47
min_obs_depth <- min(mcpbase$depthlayerno, na.rm = TRUE) #min is 1
unique_cell_xyz <- mcp_unique_cell
unique_cell_xyz$depthlayerno <- NA
unique_cell_xyz <- unique_cell_xyz[rep(seq_len(nrow(unique_cell_xyz)), each=47),] #this generates 47  repetitions of each row 47 is chosen because there are 47 sampled depth layers
unique_cell_xyz$depthlayerno <- rep(1:47) #assign 1:47 on a repeating cycle to the depth column. 47 is chosen because there are 50 depth layers
unique_cell_xyz$id_depth <- paste(unique_cell_xyz$cell_id, unique_cell_xyz$depthlayerno, sep="") #create a new column that is a unique glo_unique_cell & depth ID. This is to help remove cells later and randomise
write.csv(unique_cell_xyz, "../output/env/unique_cell_centroid_lonlat_nafo2_depth.csv", row.names = FALSE)
```



What I want to do is for each time slice, create a list of all cells (xyz), then remove the ones where observations occurred and create a .csv with the name of the level being worked on

Split the mcpbase into timeslices (year/month).
All you need is the year, month, cell_id, and depthlayerno
```{r}
cell_yymm <- subset(mcpbase, select= c(cell_id, depthlayerno, year, month)) #create a new df with just cell_id, yr, month, depthlayer no
cell_yymm$id_depth <- paste(cell_yymm$cell_id, cell_yymm$depthlayerno, sep="") #create a new column that is a unique cell_id & depth ID. This is to help remove cells later and randomise
split_cell_yymm <- split(cell_yymm, list(cell_yymm$year, cell_yymm$month), drop = TRUE)
```

Here is a loop to create a copy of the unique_cell_centroid dataframe, and give it the same name as the splitobs_cell_yymm level
.csv naming code courtesy of [nadizan/stack exchange](https://stackoverflow.com/questions/53045158/r-naming-csv-with-a-list-level-name-in-a-loop?noredirect=1#comment92990285_53045158)


```{r timesliced unique unsampled}
no_timeslices <- length(split_cell_yymm) # how many levels (time slices) are in the list
bckoutput <- "../output/bio/background/raw/" #where the files are to be saved to
for (i in 1:no_timeslices){
  timeslice <- split_cell_yymm[[i]]
  timeslice <- as.data.frame(timeslice)
  time_cell <- unique_cell_xyz
  #time_cell <- time_cell[!time_cell$id_depth %in% timeslice$id_depth, , drop = FALSE] #THIS LINE WAS TO REMOVE CELLS WHERE NO OBSERVATIONS OCCURRED BUT I SHOULD NOT HAVE DONE THIS. LEFT HERE JUST IN IN CASE 
  write.csv(time_cell, paste0(bckoutput, names(lapply(split_cell_yymm, names))[i], ".csv"), row.names = FALSE) 
}
```

Ok so now I have a bunch of .csv files containing all cells for each timeslice on xyz coordinates.
Next step is to randomly select cells in each timeslice. Make sure you generate new files for each...

load the .csvs... [code](https://www.reed.edu/data-at-reed/software/R/reading_and_writing.html)

the timesliced .csv files just created in the step above also include depths that do not exist. So you don't end up with a bunch of NA values, create a dummy file based on the environmental correlates (e.g. only have cells selected where there is a value)

load a random timeslice (in this case 2007.03)
```{r}
testbackground_all <- read.csv("../output/bio/background/raw/2007.3.csv", header = TRUE)
testbackground_all
```

First you need to create columns for the data to be added to (otherwise the loop to add the environemtnal correlates to the points data doesn't always work - notably on one PC but not another)

```{r}
testbackground_all$temp_depth <- NA
testbackground_all$temp_surface <- NA
testbackground_all$salinity_depth <- NA
testbackground_all$salinity_surface <- NA
testbackground_all$chl_depth <- NA
testbackground_all$chl_surface <- NA
testbackground_all$o2_depth <- NA
testbackground_all$o2_surface <- NA
testbackground_all$mlp_surface <- NA
testbackground_all$ssh_surface <- NA
```

Second you need to put the background (testbackground_all) into a spatialpointsdataframe.
```{r}
xy <- testbackground_all[ ,c("longitude_","latitude_m")] # This is to tell R where the coordinates are. Note that the column order needs to be longitude, latitude

#xy <- testbackground_all[ ,c("longitude_meters","latitude_meters")] # This is to tell R where the coordinates are. Note that the column order needs to be longitude, latitude
testbackground_all_sp <- SpatialPointsDataFrame(coords = xy, data = testbackground_all, proj4string = CRS("+proj=aea +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-60 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")) # The CRS is used here is for the albers equal area projection.
head(testbackground_all_sp)
```

now run the loop. Make sure that the only netcdfs in the netcdf folder are 2007_03 ones


```{r}
netcdf_list <- list.files("../data/env/netcdfbase", pattern = '*.nc', full.names = TRUE) #true means the full path is included
no_netcdf <- length(netcdf_list) #for the loop - need to know how many files to cycle through
netcdf_name <- list.files("../data/env/netcdfbase", pattern = '*.nc', full.names = FALSE) #false means the path is not included
aea <- raster("../output/env/aea.tif") 
for (i in 1:no_netcdf) {  
  print(netcdf_name[i]) #this just prints the name of the netCDF R is working one
  brkvar <- (sapply(strsplit(netcdf_name[i], "_"), "[[", 3)) # extracting the third part of the netcdf (inc.nc)
  temp_brick <- brick(netcdf_list[i], lvar = 4)
  temp_brick <- projectRaster(temp_brick, aea) 
    for (j in 1:nrow(testbackground_all_sp)) {  
      de <- testbackground_all_sp$depthlayerno[[j]]  # a variable for the observation depth layer
          if (brkvar == "temp.nc"){
              if (is.na(de)){
                testbackground_all_sp$temp_depth[j] <- NA
              } else  
                testbackground_all_sp$temp_depth[j] <- extract(x=temp_brick[[de]], y = testbackground_all_sp[j, ])
          } else if (brkvar == "salinity.nc") {
              if (is.na(de)){
                testbackground_all_sp$salinity_depth[j] <- NA
              } else  
                testbackground_all_sp$salinity_depth[j] <- extract(x=temp_brick[[de]], y = testbackground_all_sp[j, ]) 
          } else if (brkvar == "chl.nc") {
              if (is.na(de)){
                testbackground_all_sp$chl_depth[j] <- NA
              } else  
                testbackground_all_sp$chl_depth[j] <- extract(x=temp_brick[[de]], y = testbackground_all_sp[j, ]) 
          } else if (brkvar == "o2.nc") {
              if (is.na(de)){
                testbackground_all_sp$o2_depth[j] <- NA
              } else  
                testbackground_all_sp$o2_depth[j] <- extract(x=temp_brick[[de]], y = testbackground_all_sp[j, ]) 
          }
     
    }
}
write.csv(testbackground_all_sp, "../output/bio/background/background_point_na_removal.csv", row.names = FALSE)
testbackground_all <- as.data.frame(testbackground_all_sp)
head(testbackground_all_sp)
```

now remove all rows (cells) with NA values  

```{r}
testbackground_all <- read.csv("../output/bio/background_point_na_removal.csv", header = TRUE)
backna <- count(rowSums(!is.na(testbackground_all)))
backna <- sum(!complete.cases(testbackground_all)) #this just counts the number of rows that contain NA values
backna
testbackground_all <- testbackground_all[complete.cases(testbackground_all), ] #removes NA values
backna2 <- sum(complete.cases(testbackground_all)) #this just counts the number of rows that contain NA values
backna2
write.csv(testbackground_all, "../output/bio/background/background_point_na_removal_nona.csv", row.names = FALSE)
head(testbackground_all)
```

some columns need renaming into something meaningful
```{r}
colnames(testbackground_all)[colnames(testbackground_all)=="decimalLon"] <- "decimalLongitude"
colnames(testbackground_all)[colnames(testbackground_all)=="decimalLat"] <- "decimalLatitude"
colnames(testbackground_all)[colnames(testbackground_all)=="longitude_"] <- "longitude_meters"
colnames(testbackground_all)[colnames(testbackground_all)=="latitude_m"] <- "latitude_meters"
head(testbackground_all)
```

save as csv
```{r}
write.csv(testbackground_all, file = "../output/bio/background/background_point_na_removal_nona.csv", row.names = FALSE)
```

now split all the data into time slices
```{r timesliced unique unsampled}
testbackground_all <- read.csv("../output/bio/background/background_point_na_removal_nona.csv", header = TRUE)
no_timeslices <- length(split_cell_yymm) # how many levels (time slices) are in the list
bckoutput <- "../output/bio/background/raw/" #where the files are to be saved to
for (i in 1:no_timeslices){
  timeslice <- split_cell_yymm[[i]]
  timeslice <- as.data.frame(timeslice)
  time_cell <- testbackground_all
  #time_cell <- time_cell[!time_cell$id_depth %in% timeslice$id_depth, , drop = FALSE]
  write.csv(time_cell, paste0(bckoutput, names(lapply(split_cell_yymm, names))[i], ".csv"), row.names = FALSE) 
}
```

Then select 10,000 random cells per each timeslice
```{r}
bckrnd_folder <- "../output/bio/background/raw/"      # path to folder that holds multiple .csv files
bckrnd_list <- list.files(path = bckrnd_folder, pattern="*.csv") # create list of all .csv files in folder
randbckoutput <- "../output/bio/background/rand10k/" #where the files are to be saved to
# read in each .csv file in file_list and create a data frame with the same name as the .csv file
for (i in 1:length(bckrnd_list)){
  sel <- read.csv(paste(bckrnd_folder, bckrnd_list[i], sep=''))
  sel <- sample_n(sel, 10000) #where 10000 = number of rows to sample 
  write.csv(sel, file = paste0(randbckoutput, bckrnd_list[i]), row.names = FALSE) #dont need to add .csv as it's already in the filename
}
head(sel)
```

ok now i have a ton of files for random background points broken into timeslices. I'd like to add a colum for year and one for month to each of these datasets. They should be populated based on the name of the file (e.g. 1998.4.csv would have year = 1998 and month = 4)

```{r}
rand_folder <- "../output/bio/background/rand10k/"      # path to folder that holds multiple .csv files
rand_list <- list.files(path = rand_folder, pattern="*.csv") # create list of all .csv files in folder
randbckoutput <- "../output/bio/background/rand10k/" #where the files are to be saved to
for(i in 1:length(rand_list)){
  randtemp <- read.csv(paste(rand_folder, rand_list[i], sep=''))
  randtemp$year <- substr(rand_list[i], 1, 4) #the year is characters 1-4 in the filename
  randtemp$month <- substr(rand_list[i], 6, 7) #the month is characters 6 & 7 in the filename (except where the month is a single digit)
  write.csv(randtemp, file = paste0(randbckoutput, bckrnd_list[i]), row.names = FALSE) #dont need to add .csv as it's already in the filename
}
head(randtemp)
```

note that in the output temp file in R shows a . after the month if that month is a single digit. The .csv outputs don't 

# populating the background points data with... data

ok now you can start to populate the rest of the background point information. The background points file should mirror as much as possible the occurence file. Maybe it would be best to merge the files together, and them split them again later? would save looping everything to repeat the process through multiple files...

```{r}
rand_folder <- "../output/bio/background/rand10k/"      # path to folder that holds multiple .csv files
filenames  <- list.files(path = rand_folder, pattern="*.csv", full.names = TRUE) 
background_all <- rbindlist(lapply(filenames,fread, fill = TRUE))
write.csv(background_all, "../output/bio/background_all.csv", row.names = FALSE)
head(background_all)
```

check for unique years
```{r}
back_yr_uniq <- unique(background_all$year)
back_yr_uniq
```
check for unique months
```{r}
back_mt_uniq <- unique(background_all$month)
back_mt_uniq
```

check number of background points in total 
```{r}
back_rowcount <- nrow(background_all)
back_rowcount
```

ok lets start by getting rid of some of the duplicated columns right now (put in by arc, various processes..)

```{r}
head(background_all)
```
remove
FID
optional


```{r}
background_all <- subset(background_all, select = -c(optional, FID))
head(background_all)
```

# what data to add

get the columns from the occurence data so you know what to add
```{r}
occ_column <- colnames(data_aea)
occ_column
```

ok so have cell_id, year, month, decimalLongitude.1(meters - in background_all its longitude), decimalLatitude.1 (meters - in background_all its latitude), depth_layer_no.

# create unique observation ID

Easy first one - add an id based on id_depth
```{r}
names(background_all)[names(background_all)=="id_depth"] <- "id"
write.csv(background_all, "../output/bio/background_all.csv", row.names = FALSE)
head(background_all)
```


#original scientific name

create a column called "originalscientificname" and fill it with "Mallotus villosus ab"

```{r}
background_all$originalscientificname <- "Mallotus villosus ab"
write.csv(background_all, "../output/bio/background_all_lonlat_nafo2_sciname.csv")
head(background_all)
```

# Add AMO Data
The data has been prepared as per the environmental_data_preperation.Rmd, and biological_data_prep.Rmd

So you want..
- AMO value at sampling month/year
- AMO value at previous sampling month/year
- AMO phase at previous winter (because the winter values are thought to be a major driver of ocean conditions in the following spring, summer, and autumn)

Load the prepared amo_prev.csv file with has the amo at different year/month steps
```{r}
amo_prev <- read.csv("../output/env/amo_prev.csv", header = TRUE)
head(amo_prev)
```

now match the year and the month in the two dataframes, and populate the background points with amo_sample
```{r}
background_all <- merge(x = background_all, y = amo_prev[ , c("year", "month", "amo_sample")], by = c("year", "month"), all.x = TRUE)
head(background_all)
```

ok next - AMO value at previous sampling month/year 
```{r}
background_all <- merge(x = background_all, y = amo_prev[ , c("year", "month", "amo_prev")], by = c("year", "month"), all.x = TRUE)
head(background_all)
```

and now - AMO phase at previous winter (because the winter values are thought to be a major driver of ocean conditions in the following spring, summer, and autumn) (note they are in a different .csv)

```{r}
amo_winter <- read.csv("../output/env/amo_winter.csv")
background_all <- merge(x = background_all, y = amo_winter[ , c("year", "WinterAvg.Shifted")], by = c("year"), all.x = TRUE)
colnames(background_all)[colnames(background_all)=="WinterAvg.Shifted"] <- "amo_winter" #change the colname
head(background_all)
```

ok and write a csv
```{r}
write.csv(data_aea,"../output/bio/background_all_lonlat_nafo2_sciname_amo.csv", row.names = FALSE)
```


# NAO values
The data has been prepared as per the environmental_data_preperation.Rmd and biological_data_prep.Rmd

So you want..
- NAO value at sampling month/year
- NAO value at previous sampling month/year
- NAO phase at previous winter (because the winter values are thought to be a major driver of ocean conditions in the following spring, summer, and autumn)

Load the prepared nao_prev.csv file with has the amo at different year/month steps
```{r}
nao_prev <- read.csv("../output/env/nao_prev.csv", header = TRUE)
head(nao_prev)
```

now match the year and the month in the two dataframes, and populate the background points with nao_sample

```{r}
background_all <- merge(x = background_all, y = nao_prev[ , c("year", "month", "nao_sample")], by = c("year", "month"), all.x = TRUE)
background_all
```

ok next - NAO value at previous sampling month/year
```{r}
background_all <- merge(x = background_all, y = nao_prev[ , c("year", "month", "nao_prev")], by = c("year", "month"), all.x = TRUE)
background_all
```

and now - NAO phase at previous winter (because the winter values are thought to be a major driver of ocean conditions in the following spring, summer, and autumn). The data is in a different .CSV

```{r}
nao_winter <- read.csv("../output/env/nao_winter.csv")
background_all <- merge(x = background_all, y = nao_winter[ , c("year", "WinterAvg.Shifted")], by = c("year"), all.x = TRUE)
colnames(background_all)[colnames(background_all)=="WinterAvg.Shifted"] <- "nao_winter" #change the colname
background_all
```

and write .csv
```{r}
write.csv(background_all,"../output/bio/background_all_lonlat_nafo2_sciname_amo_nao.csv", row.names = FALSE)
```

still need
-bottom_depth
-depth_layer
-total_cell_obs
-yymm_cell_obs

#depth layer

the depth layers in the netCDF are literal slices. Currently I have listed the depthlayerno (which is literally which layer in the netcdf). I should also add the depth_layer name which gives the depth the slice is supposed to represent. Data is stored in depth_bin.csv

load the depth bin csv
```{r}
depthbin <- read.csv("../data/env/depth_bin.csv", header = TRUE) 
head(depthbin)
```

so basically what i want to do is, where depthbin$layer_no = background_all$depthlayerno, extract depthbin$layer_name and add to background_all$depth_layer. Can use the merge function.
First need to change depthbin col names to match background_all

```{r}
names(depthbin)[names(depthbin)=="layer_no"] <- "depthlayerno"
names(depthbin)[names(depthbin)=="layer_name"] <- "depth_layer" #this one is just so i don't have to change the column name in the background_all file later
head(depthbin)
```


```{r}
background_all <- merge(background_all, y = depthbin[ , c("depthlayerno", "depth_layer")], by = "depthlayerno", all.x = TRUE)
head(background_all)
```

just check some random rows...

```{r}
randcheck <- background_all[sample(nrow(background_all), 10), ]
randcheck
```
all good - save to .csv

```{r}
write.csv(background_all, "../output/bio/background_all_lonlat_nafo2_sciname_amo_nao_depthname.csv", row.names = FALSE)
```


# Bathymetric

bottom depth from [GEBCO](www.gebco.net)

```{r}
bat <-readGEBCO.bathy("../data/env/GEBCO_2014_2D_-70.0_34.0_-43.0_70.0.nc")
summary(bat)
```

create pretty custom color palette and then plot the bathy layer with species data
```{r}
blues <- colorRampPalette(c("lightblue", "cadetblue2", "cadetblue1", "white"))
plot(bat, n = 1, image = TRUE, bpal = blues(100))
points(background_all$decimalLongitude, background_all$decimalLatitude, pch = 20, col = "yellow") #note these are both the wgs84 images
```
lovely jubbely

and now get depth... note all the points and bathy data is wgs!

```{r}
background_all <- read.csv("../output/bio/background_all_lonlat_nafo2_sciname_amo_nao_depthname.csv", header = TRUE)
background_all$bottom_depth <- get.depth(bat, background_all[ ,8:9], locator = FALSE)
background_all$bottom_depth <- background_all$bottom_depth[, 3] #this step is because get.bath actually produces it's own dataframe with long, lat, and depth. This is extracting the depth column and deplacing data_aea$bottom_depth with the correct data.
write.csv(background_all,"../output/bio/background_all_lonlat_nafo2_sciname_amo_nao_depthname_bath.csv", row.names = FALSE)
head(background_all)
```

# observations by cell (in total, and also by time step)

as the title says... how many observations happeend in each cell:
- in total


```{r observation by cell - total}
obs_by_cell_back <- count(background_all, "cell_id")
write.csv(obs_by_cell_back, file = "../output/bio/back_no_observations_cell.csv")
head(obs_by_cell_back)
```

- in each yyyymm
```{r}
obs_cell_yymm_back <- count(background_all, c("cell_id", "year", "month"))
write.csv(obs_cell_yymm_back, file = "../output/bio/back_no_observations_cell_yymm.csv")
head(obs_cell_yymm_back)
```

- in each yyyymm at depth
```{r}
obs_cell_yymm_depth_back <- count(background_all, c("cell_id", "year", "month", "depthlayerno"))
write.csv(obs_cell_yymm_depth_back, file = "../output/bio/back_no_observations_cell_yymm_depth.csv")
head(obs_cell_yymm_depth_back)
```


Now attach these values 

total obs for the whole time period
```{r}
background_all <- merge(background_all, obs_by_cell_back, by="cell_id")
head(background_all)
```

rename freq column to something meaningful
```{r}
names(background_all)[names(background_all)=="freq"] <- "total_cell_obs_xy"
head(background_all)
```

just check some random rows...

```{r}
randcheckcell <- background_all[sample(nrow(background_all), 10), ]
randcheckcell
```
all good

time-sliced
```{r}
background_all <- merge(background_all, obs_cell_yymm_back, by.x=c("cell_id", "year", "month"), by.y=c("cell_id", "year", "month"))
names(background_all)[names(background_all)=="freq"] <- "total_cell_obs_xyt"
background_all <- merge(background_all, obs_cell_yymm_depth_back, by.x=c("cell_id", "year", "month", "depthlayerno"), by.y=c("cell_id", "year", "month", "depthlayerno"))
names(background_all)[names(background_all)=="freq"] <- "total_cell_obs_xyzt"
write.csv(background_all,"../output/bio/background_all_lonlat_nafo2_sciname_amo_nao_depthname_bath_cellobs.csv", row.names = FALSE)
head(background_all)
```

and just check some random rows...

```{r}
randcheckyymm <- background_all[sample(nrow(background_all), 10), ]
randcheckyymm
```

# attach environmental correlates

This loop runs through the netcdf files and then looks for which rows in data_aea it should extract the value to point from, and at what depth (netcDF layer)

First you need to create columns for the data to be added to (otherwise the loop doesn't always work - notably on one PC but not another)

```{r}
background_all <- read.csv("../output/bio/background_all_lonlat_nafo2_sciname_amo_nao_depthname_bath_cellobs.csv", header = TRUE)
background_all$temp_depth <- NA
background_all$temp_surface <- NA
background_all$salinity_depth <- NA
background_all$salinity_surface <- NA
background_all$chl_depth <- NA
background_all$chl_surface <- NA
background_all$o2_depth <- NA
background_all$o2_surface <- NA
background_all$mlp_surface <- NA
background_all$ssh_surface <- NA
```


Second you need to put the background (background_all) into a spatialpointsdataframe.
```{r}
<<<<<<< HEAD
background_all2 <- background_all[background_all$year == 2009, ]
=======
background_all2 <- background_all[background_all$year == 1999 & background_all$month == 7, ]
>>>>>>> 046838528c0b1f64fae8e0f2134dc995971d5a11
xy <- background_all2[ ,c("longitude_meters","latitude_meters")] # This is to tell R where the coordinates are. Note that the column order needs to be longitude, latitude

#xy <- background_all[ ,c("longitude_meters","latitude_meters")] # This is to tell R where the coordinates are. Note that the column order needs to be longitude, latitude
background_all_sp <- SpatialPointsDataFrame(coords = xy, data = background_all2, proj4string = CRS("+proj=aea +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-60 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")) # The CRS is used here is for the albers equal area projection.
head(background_all_sp)
```


```{r}
netcdf_list <- list.files("../data/env/netcdf", pattern = '*.nc', full.names = TRUE) #true means the full path is included
no_netcdf <- length(netcdf_list) #for the loop - need to know how many files to cycle through
netcdf_name <- list.files("../data/env/netcdf", pattern = '*.nc', full.names = FALSE) #false means the path is not included
aea <- raster("../output/env/aea.tif") 
for (i in 1:no_netcdf) {  
  print(netcdf_name[i]) #this just prints the name of the netCDF R is working one
  brkyr <- as.integer(sapply(strsplit(netcdf_name[i], "_"), "[[", 1)) # extracting the first part of the netcdf filename (which is the year)
  brkmth <- as.integer(sapply(strsplit(netcdf_name[i], "_"), "[[", 2)) # extracting the second part of the netcdf filename (which is the month)
  brkvar <- (sapply(strsplit(netcdf_name[i], "_"), "[[", 3)) # extracting the third part of the netcdf (inc.nc)
  temp_brick <- brick(netcdf_list[i], lvar = 4)
  temp_brick <- projectRaster(temp_brick, aea) 
    for (j in 1:nrow(background_all_sp)) {  
      de <- background_all_sp$depthlayerno[[j]]  # a variable for the observation depth layer
      yr <- (background_all_sp$year[j])  # a variable for the observation year
      mth <- (background_all_sp$month[j])  # a variable for the observation month
          if (brkyr == yr & brkmth == mth & brkvar == "temp.nc"){
              background_all_sp$temp_surface[j] <- extract(x=temp_brick[[1]], y = background_all_sp[j, ]) 
              if (is.na(de)){
                background_all_sp$temp_depth[j] <- NA
              } else  
                background_all_sp$temp_depth[j] <- extract(x=temp_brick[[de]], y = background_all_sp[j, ])
          } else if (brkyr == yr & brkmth == mth & brkvar == "salinity.nc") {
              background_all_sp$salinity_surface[j] <- extract(x=temp_brick[[1]], y = background_all_sp[j, ]) 
              if (is.na(de)){
                background_all_sp$salinity_depth[j] <- NA
              } else  
                background_all_sp$salinity_depth[j] <- extract(x=temp_brick[[de]], y = background_all_sp[j, ]) 
          } else if (brkyr == yr & brkmth == mth & brkvar == "chl.nc") {
              background_all_sp$chl_surface[j] <- extract(x=temp_brick[[1]], y = background_all_sp[j, ]) 
              if (is.na(de)){
                background_all_sp$chl_depth[j] <- NA
              } else  
                background_all_sp$chl_depth[j] <- extract(x=temp_brick[[de]], y = background_all_sp[j, ]) 
          } else if (brkyr == yr & brkmth == mth & brkvar == "o2.nc") {
              background_all_sp$o2_surface[j] <- extract(x=temp_brick[[1]], y = background_all_sp[j, ]) 
              if (is.na(de)){
                background_all_sp$o2_depth[j] <- NA
              } else  
                background_all_sp$o2_depth[j] <- extract(x=temp_brick[[de]], y = background_all_sp[j, ]) 
          } else if (brkyr == yr & brkmth == mth & brkvar == "mlp.nc") {
              background_all_sp$mlp_surface[j] <- extract(x=temp_brick[[1]], y = background_all_sp[j, ])
          } else if (brkyr == yr & brkmth == mth & brkvar == "ssh.nc") {
              background_all_sp$ssh_surface[j] <- extract(x=temp_brick[[1]], y = background_all_sp[j, ]) 
            
          }
     
    }
}
<<<<<<< HEAD
write.csv(background_all_sp, "../output/bio/background_complete_2009.csv", row.names = FALSE)
=======
write.csv(background_all_sp, "../output/bio/background_all_complete.csv", row.names = FALSE)
>>>>>>> 046838528c0b1f64fae8e0f2134dc995971d5a11
background_all2 <- as.data.frame(background_all_sp)
head(background_all_sp)
```


and just check some random rows...

```{r}
randcheck <- background_all[sample(nrow(background_all), 5), ]
randcheck
```

```{r}
randchkxy <- randcheck[ , c("longitude_meters", "latitude_meters")] 
randchsp <- SpatialPointsDataFrame(coords = randchkxy, data = randcheck, proj4string = CRS("+proj=aea +lat_1=50 +lat_2=70 +lat_0=40 +lon_0=-60 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")) # The CRS is used here is for the albers equal area projection.

#row 1
temp_brick <-  brick("../data/env/netcdf/2002_03_chl.nc", lvar = 4)
t200203chl <- projectRaster(temp_brick, aea)
chls <- extract(x=t200203chl[[1]], y = randchsp[1, ]) 
chls
chld <- extract(x=t200203chl[[23]], y = randchsp[1, ]) 
chld 
temp_brick <-  brick("../data/env/netcdf/2002_03_mlp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
mlps <- extract(x=tempbrick[[1]], y = randchsp[1, ]) 
mlps
temp_brick <-  brick("../data/env/netcdf/2002_03_o2.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
o2s <- extract(x=tempbrick[[1]], y = randchsp[1, ]) 
o2s
o2d <- extract(x=tempbrick[[23]], y = randchsp[1, ]) 
o2d
temp_brick <-  brick("../data/env/netcdf/2002_03_salinity.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sals <- extract(x=tempbrick[[1]], y = randchsp[1, ]) 
sals
sald <- extract(x=tempbrick[[23]], y = randchsp[1, ]) 
sald
temp_brick <-  brick("../data/env/netcdf/2002_03_temp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
temps <- extract(x=tempbrick[[1]], y = randchsp[1, ]) 
temps
tempd <- extract(x=tempbrick[[23]], y = randchsp[1, ]) 
tempd
temp_brick <-  brick("../data/env/netcdf/2002_03_ssh.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sshs <- extract(x=tempbrick[[1]], y = randchsp[1, ]) 
sshs

#row 2
temp_brick <-  brick("../data/env/netcdf/2007_07_chl.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
tempbrick
chls <- extract(x=tempbrick[[1]], y = randchsp[2, ]) 
chls
chld <- extract(x=tempbrick[[13]], y = randchsp[2, ]) 
chld 
temp_brick <-  brick("../data/env/netcdf/2007_07_mlp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
mlps <- extract(x=tempbrick[[1]], y = randchsp[2, ]) 
mlps
temp_brick <-  brick("../data/env/netcdf/2007_07_o2.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
o2s <- extract(x=tempbrick[[1]], y = randchsp[2, ]) 
o2s
o2d <- extract(x=tempbrick[[13]], y = randchsp[2, ]) 
o2d
temp_brick <-  brick("../data/env/netcdf/2007_07_salinity.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sals <- extract(x=tempbrick[[1]], y = randchsp[2, ]) 
sals
sald <- extract(x=tempbrick[[13]], y = randchsp[2, ]) 
sald
temp_brick <-  brick("../data/env/netcdf/2007_07_temp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
temps <- extract(x=tempbrick[[1]], y = randchsp[2, ]) 
temps
tempd <- extract(x=tempbrick[[13]], y = randchsp[2, ]) 
tempd
temp_brick <-  brick("../data/env/netcdf/2007_07_ssh.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sshs <- extract(x=tempbrick[[1]], y = randchsp[2, ]) 
sshs

#row 3
temp_brick <-  brick("../data/env/netcdf/2011_08_chl.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
tempbrick
chls <- extract(x=tempbrick[[1]], y = randchsp[3, ]) 
chls
chld <- extract(x=tempbrick[[13]], y = randchsp[3, ]) 
chld 
temp_brick <-  brick("../data/env/netcdf/2011_08_mlp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
mlps <- extract(x=tempbrick[[1]], y = randchsp[3, ]) 
mlps
temp_brick <-  brick("../data/env/netcdf/2011_08_o2.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
o2s <- extract(x=tempbrick[[1]], y = randchsp[3, ]) 
o2s
o2d <- extract(x=tempbrick[[13]], y = randchsp[3, ]) 
o2d
temp_brick <-  brick("../data/env/netcdf/2011_08_salinity.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sals <- extract(x=tempbrick[[1]], y = randchsp[3, ]) 
sals
sald <- extract(x=tempbrick[[13]], y = randchsp[3, ]) 
sald
temp_brick <-  brick("../data/env/netcdf/2011_08_temp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
temps <- extract(x=tempbrick[[1]], y = randchsp[3, ]) 
temps
tempd <- extract(x=tempbrick[[13]], y = randchsp[3, ]) 
tempd
temp_brick <-  brick("../data/env/netcdf/2011_08_ssh.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sshs <- extract(x=tempbrick[[1]], y = randchsp[3, ]) 
sshs

#row 4
temp_brick <-  brick("../data/env/netcdf/2000_05_chl.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
tempbrick
chls <- extract(x=tempbrick[[1]], y = randchsp[4, ]) 
chls
chld <- extract(x=tempbrick[[2]], y = randchsp[4, ]) 
chld 
temp_brick <-  brick("../data/env/netcdf/2000_05_mlp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
mlps <- extract(x=tempbrick[[1]], y = randchsp[4, ]) 
mlps
temp_brick <-  brick("../data/env/netcdf/2000_05_o2.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
o2s <- extract(x=tempbrick[[1]], y = randchsp[4, ]) 
o2s
o2d <- extract(x=tempbrick[[2]], y = randchsp[4, ]) 
o2d
temp_brick <-  brick("../data/env/netcdf/2000_05_salinity.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sals <- extract(x=tempbrick[[1]], y = randchsp[4, ]) 
sals
sald <- extract(x=tempbrick[[2]], y = randchsp[4, ]) 
sald
temp_brick <-  brick("../data/env/netcdf/2000_05_temp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
temps <- extract(x=tempbrick[[1]], y = randchsp[4, ]) 
temps
tempd <- extract(x=tempbrick[[2]], y = randchsp[4, ]) 
tempd
temp_brick <-  brick("../data/env/netcdf/2000_05_ssh.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sshs <- extract(x=tempbrick[[1]], y = randchsp[4, ]) 
sshs

#row 5
temp_brick <-  brick("../data/env/netcdf/2012_06_chl.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
tempbrick
chls <- extract(x=tempbrick[[1]], y = randchsp[5, ]) 
chls
chld <- extract(x=tempbrick[[25]], y = randchsp[5, ]) 
chld 
temp_brick <-  brick("../data/env/netcdf/2012_06_mlp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
mlps <- extract(x=tempbrick[[1]], y = randchsp[5, ]) 
mlps
temp_brick <-  brick("../data/env/netcdf/2012_06_o2.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
o2s <- extract(x=tempbrick[[1]], y = randchsp[5, ]) 
o2s
o2d <- extract(x=tempbrick[[25]], y = randchsp[5, ]) 
o2d
temp_brick <-  brick("../data/env/netcdf/2012_06_salinity.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sals <- extract(x=tempbrick[[1]], y = randchsp[5, ]) 
sals
sald <- extract(x=tempbrick[[25]], y = randchsp[5, ]) 
sald
temp_brick <-  brick("../data/env/netcdf/2012_06_temp.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
temps <- extract(x=tempbrick[[1]], y = randchsp[5, ]) 
temps
tempd <- extract(x=tempbrick[[25]], y = randchsp[5, ]) 
tempd
temp_brick <-  brick("../data/env/netcdf/2012_06_ssh.nc", lvar = 4)
tempbrick <- projectRaster(temp_brick, aea)
sshs <- extract(x=tempbrick[[1]], y = randchsp[5, ]) 
sshs
```

it worked!
Double-check for NA values

```{r}
backna <- sum(!complete.cases(background_all)) #this just counts the number of rows that contain NA values
backna
```
None!


# observation occurrence yes or no

create a new column for Occurence, and populate with "0" (because these are all background)
```{r}
background_all$occurrence <- "0"
write.csv(background_all, "../output/bio/background_complete_obs.csv", row.names = FALSE)
head(background_all)
```

# add a col for temp variable in celsius (every-day person readable)

Uses the weathermetrics package

```{r}
background_all$temp_celsius_depth <- kelvin.to.celsius(background_all$temp_depth, round = 4) # 4 is the number of decimal places. the values are expressed to 4
background_all$temp_celsius_surface <- kelvin.to.celsius(background_all$temp_surface, round = 4)
write.csv(background_all, "../output/bio/background_complete_obs_cels.csv", row.names = FALSE)
head(background_all)
```
